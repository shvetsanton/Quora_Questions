{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "np.random.seed(1337)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../quara_questions/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'google/GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'data/train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'data/test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Indexing word vectors.')\n",
    "# embeddings_index = {}\n",
    "# f = codecs.open(os.path.join(GOOGLE_DIR), encoding='utf-8')\n",
    "# for line in f:\n",
    "#     values = line.split(' ')\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the train and test questions into list of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 404290 texts.\n",
      "Found 2345796 texts.\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []  # list of label ids\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts.' % len(texts_1))\n",
    "\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_labels = []  # list of label ids\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_labels.append(values[0])\n",
    "print('Found %s texts.' % len(test_texts_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using keras tokenizer to tokenize the text and then do padding the sentences to 30 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120499 unique tokens.\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_labels = np.array(test_labels)\n",
    "del test_sequences_1\n",
    "del test_sequences_2\n",
    "del sequences_1\n",
    "del sequences_2\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embedding matrix where each row corresponds to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 61789\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Preparing embedding matrix.')\n",
    "# # prepare embedding matrix\n",
    "# nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "# embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= nb_words:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "# print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Train/Validation Data\n",
    "# perm = np.random.permutation(len(data_1))\n",
    "# idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "# idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "# data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "# data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "# labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "# data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "# data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "# labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "# weight_val = np.ones(len(labels_val))\n",
    "# if re_weight:\n",
    "#     weight_val *= 0.472001959\n",
    "#     weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm = 128\n",
    "num_dense = 128\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashvets/miniconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[/input_1,..., outputs=sigmoid.0)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 300)       36150000    input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 128)           219648      embedding_1[0][0]                \n",
      "                                                                   embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 256)           0           lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 256)           1024        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 64)            16448       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 64)            256         dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             65          batch_normalization_2[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 36,387,441\n",
      "Trainable params: 236,801\n",
      "Non-trainable params: 36,150,640\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture #\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "#x1 = Conv1D(128, 3, activation='relu')(embedded_sequences_1)\n",
    "# x1 = MaxPooling1D(10)(x1)\n",
    "# x1 = Flatten()(x1)\n",
    "# x1 = Dense(64, activation='relu')(x1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_1)\n",
    "#y1 = Conv1D(128, 3, activation='relu')(embedded_sequences_2)\n",
    "# y1 = MaxPooling1D(10)(y1)\n",
    "# y1 = Flatten()(y1)\n",
    "# y1 = Dense(64, activation='relu')(y1)\n",
    "# y1 = Dropout(0.2)(y1)\n",
    "\n",
    "merged = concatenate([x1,y1])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(input=[sequence_1_input,sequence_2_input], output=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re_weight = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if re_weight:\n",
    "#     class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "# else:\n",
    "#     class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_128_128_0.20_0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400247 samples, validate on 4043 samples\n",
      "Epoch 1/20\n",
      "400247/400247 [==============================] - 2042s - loss: 0.5922 - acc: 0.6892 - val_loss: 0.5660 - val_acc: 0.7190\n",
      "Epoch 2/20\n",
      "400247/400247 [==============================] - 2046s - loss: 0.5436 - acc: 0.7247 - val_loss: 0.5200 - val_acc: 0.7418\n",
      "Epoch 3/20\n",
      "400247/400247 [==============================] - 2040s - loss: 0.5250 - acc: 0.7391 - val_loss: 0.5079 - val_acc: 0.7497\n",
      "Epoch 4/20\n",
      "400247/400247 [==============================] - 2040s - loss: 0.5110 - acc: 0.7483 - val_loss: 0.4954 - val_acc: 0.7551\n",
      "Epoch 5/20\n",
      "400247/400247 [==============================] - 2041s - loss: 0.4993 - acc: 0.7563 - val_loss: 0.4984 - val_acc: 0.7546\n",
      "Epoch 6/20\n",
      "400247/400247 [==============================] - 2044s - loss: 0.4884 - acc: 0.7634 - val_loss: 0.4882 - val_acc: 0.7658\n",
      "Epoch 7/20\n",
      "400247/400247 [==============================] - 2037s - loss: 0.4790 - acc: 0.7694 - val_loss: 0.4861 - val_acc: 0.7727\n",
      "Epoch 8/20\n",
      "400247/400247 [==============================] - 2048s - loss: 0.4695 - acc: 0.7751 - val_loss: 0.4830 - val_acc: 0.7739\n",
      "Epoch 9/20\n",
      "400247/400247 [==============================] - 2053s - loss: 0.4609 - acc: 0.7811 - val_loss: 0.4837 - val_acc: 0.7776\n",
      "Epoch 10/20\n",
      "400247/400247 [==============================] - 2052s - loss: 0.4525 - acc: 0.7850 - val_loss: 0.4745 - val_acc: 0.7724\n",
      "Epoch 11/20\n",
      "400247/400247 [==============================] - 2050s - loss: 0.4447 - acc: 0.7898 - val_loss: 0.4710 - val_acc: 0.7858\n",
      "Epoch 12/20\n",
      "400247/400247 [==============================] - 2055s - loss: 0.4372 - acc: 0.7937 - val_loss: 0.4793 - val_acc: 0.7786\n",
      "Epoch 13/20\n",
      "400247/400247 [==============================] - 2058s - loss: 0.4306 - acc: 0.7976 - val_loss: 0.4835 - val_acc: 0.7729\n",
      "Epoch 14/20\n",
      "400247/400247 [==============================] - 2056s - loss: 0.4247 - acc: 0.8005 - val_loss: 0.4714 - val_acc: 0.7870\n",
      "Epoch 15/20\n",
      "400247/400247 [==============================] - 2058s - loss: 0.4184 - acc: 0.8038 - val_loss: 0.4762 - val_acc: 0.7856\n",
      "Epoch 16/20\n",
      "400247/400247 [==============================] - 2064s - loss: 0.4140 - acc: 0.8059 - val_loss: 0.4745 - val_acc: 0.7883\n",
      "Epoch 17/20\n",
      "400247/400247 [==============================] - 2062s - loss: 0.4086 - acc: 0.8086 - val_loss: 0.4689 - val_acc: 0.7890\n",
      "Epoch 18/20\n",
      "305152/400247 [=====================>........] - ETA: 488s - loss: 0.4027 - acc: 0.8118"
     ]
    }
   ],
   "source": [
    "print(STAMP)\n",
    "\n",
    "#early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1,data_2], labels, validation_split=VALIDATION_SPLIT, epochs=20, batch_size=1024)\n",
    "        \n",
    "        #class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit([data_1,data_2], labels, validation_split=VALIDATION_SPLIT, nb_epoch=1, batch_size=1024, shuffle=True, verbose=1)\n",
    "#preds = model.predict([test_data_1, test_data_2])\n",
    "#print(preds.shape)\n",
    "\n",
    "#out_df = pd.DataFrame({\"test_id\":test_labels, \"is_duplicate\":preds.ravel()})\n",
    "#out_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([test_data_1, test_data_2], batch_size=1024, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1], batch_size=1024, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
